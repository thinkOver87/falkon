

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>falkon.mmv_ops &mdash; falkon 0.6.2 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="falkon.sparse" href="sparse.html" />
    <link rel="prev" title="falkon.ooc_ops" href="outofcore.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> falkon
          

          
          </a>

          
            
            
              <div class="version">
                0.6.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#supported-platforms">Supported Platforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#pytorch-and-cuda">PyTorch and CUDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#intel-mkl">Intel MKL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#keops">KeOps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing">Installing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#testing-the-installation">Testing the installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../get_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#passing-options">Passing Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html#more-examples">More Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/simple_regression.html">Falkon Regression Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Pre-process-the-data">Pre-process the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Create-the-Falkon-model">Create the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Training-the-model">Training the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/simple_regression.html#Evaluating-model-performance">Evaluating model performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/logistic_falkon.html">Introducing Logistic Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Define-the-Falkon-model">Define the Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Define-Logistic-Falkon-model">Define Logistic Falkon model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Train-both-models">Train both models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/logistic_falkon.html#Testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/logistic_falkon.html#Plot-predictions">Plot predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/hyperparameter_tuning.html">Hyperparameter Tuning with Falkon</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Load-the-data">Load the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Split-into-training-and-test-sets">Split into training and test sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Data-Preprocessing">Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Search-for-the-optimal-parameters">Search for the optimal parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Evaluating-the-model">Evaluating the model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hyperparameter_tuning.html#Plot-grid-search-results">Plot grid-search results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/performance_tuning.html">Training on the GPU</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="models.html">falkon.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#falkon">Falkon</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#logisticfalkon">LogisticFalkon</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#incorefalkon">InCoreFalkon</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="kernels.html">falkon.kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="kernels.html#kernel">Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="kernels.html#radial-kernels">Radial kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="kernels.html#gaussian-kernel">Gaussian kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="kernels.html#laplacian-kernel">Laplacian kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="kernels.html#dot-product-kernels">Dot-Product kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="kernels.html#polynomial-kernel">Polynomial kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="kernels.html#linear-kernel">Linear kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="kernels.html#sigmoid-kernel">Sigmoid kernel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="options.html">falkon.options</a><ul>
<li class="toctree-l3"><a class="reference internal" href="options.html#falkonoptions">FalkonOptions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gsc_losses.html">falkon.gsc_losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#loss">Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#logistic-loss">Logistic loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="gsc_losses.html#weighted-binary-cross-entropy-loss">Weighted binary cross entropy loss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="preconditioner.html">falkon.preconditioner</a><ul>
<li class="toctree-l3"><a class="reference internal" href="preconditioner.html#preconditioner">Preconditioner</a></li>
<li class="toctree-l3"><a class="reference internal" href="preconditioner.html#cholesky-preconditioners">Cholesky preconditioners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="preconditioner.html#falkonpreconditioner">FalkonPreconditioner</a></li>
<li class="toctree-l4"><a class="reference internal" href="preconditioner.html#logisticpreconditioner">LogisticPreconditioner</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optimization.html">falkon.optim</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optimization.html#optimizer">Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="optimization.html#conjugate-gradient-methods">Conjugate gradient methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optimization.html#conjugategradient">ConjugateGradient</a></li>
<li class="toctree-l4"><a class="reference internal" href="optimization.html#falkonconjugategradient">FalkonConjugateGradient</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="outofcore.html">falkon.ooc_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="outofcore.html#gpu-cholesky">gpu_cholesky</a></li>
<li class="toctree-l3"><a class="reference internal" href="outofcore.html#gpu-lauum">gpu_lauum</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">falkon.mmv_ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-keops-mmv">run_keops_mmv</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmm-cpu">fmm_cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmm-cpu-sparse">fmm_cpu_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmm-cuda">fmm_cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmm-cuda-sparse">fmm_cuda_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmmv-cpu">fmmv_cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmmv-cpu-sparse">fmmv_cpu_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fdmmv-cpu">fdmmv_cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fdmmv-cpu-sparse">fdmmv_cpu_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmmv-cuda">fmmv_cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fmmv-cuda-sparse">fmmv_cuda_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fdmmv-cuda">fdmmv_cuda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fdmmv-cuda-sparse">fdmmv_cuda_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#incore-fmmv">incore_fmmv</a></li>
<li class="toctree-l3"><a class="reference internal" href="#incore-fdmmv">incore_fdmmv</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparse.html">falkon.sparse</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sparse.html#sparsetensor">SparseTensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparse.html#sparse-operations">Sparse operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="center_selector.html">falkon.center_selection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#centerselector">CenterSelector</a></li>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#uniformselector">UniformSelector</a></li>
<li class="toctree-l3"><a class="reference internal" href="center_selector.html#fixedselector">FixedSelector</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">falkon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">API Reference</a> &raquo;</li>
        
      <li>falkon.mmv_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api_reference/mmv_ops.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="falkon-mmv-ops">
<h1>falkon.mmv_ops<a class="headerlink" href="#falkon-mmv-ops" title="Permalink to this headline">¶</a></h1>
<p>The algorithms to compute kernels and kernel-vector products blockwise on GPUs and CPU. The algorithms in this module
are kernel agnostic. Refer to <a class="reference internal" href="kernels.html#module-falkon.kernels" title="falkon.kernels"><code class="xref py py-mod docutils literal notranslate"><span class="pre">falkon.kernels</span></code></a> for the actual kernel implementations.</p>
<p>The KeOps wrapper only supports the <cite>mmv</cite> operation (kernel-vector products). The matrix-multiplication implementations
instead support three different operations:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>mm</cite> which calculates the full kernel</p></li>
<li><p><cite>mmv</cite> which calculates kernel-vector products</p></li>
<li><p><cite>dmmv</cite> which calculates double kernel-vector products (which are operations like <span class="math notranslate nohighlight">\(K^\top (K v)\)</span> where
<span class="math notranslate nohighlight">\(K\)</span> is a kernel matrix and <span class="math notranslate nohighlight">\(v\)</span> is a vector).</p></li>
</ul>
</div></blockquote>
<span class="target" id="module-falkon.mmv_ops"></span><div class="section" id="run-keops-mmv">
<h2>run_keops_mmv<a class="headerlink" href="#run-keops-mmv" title="Permalink to this headline">¶</a></h2>
<p>A thin wrapper to KeOps is provided to allow for block-splitting and multiple GPU usage. This only supports
kernel-vector products.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.keops.run_keops_mmv">
<code class="sig-prename descclassname">falkon.mmv_ops.keops.</code><code class="sig-name descname">run_keops_mmv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">other_vars</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">formula</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">aliases</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">axis</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">reduction</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'Sum'</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions">falkon.options.FalkonOptions</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.keops.run_keops_mmv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fmm-cpu">
<h2>fmm_cpu<a class="headerlink" href="#fmm-cpu" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel calculation on the CPU.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmm_cpu.fmm_cpu">
<code class="sig-prename descclassname">falkon.mmv_ops.fmm_cpu.</code><code class="sig-name descname">fmm_cpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="p">:</span> <span class="n"><a class="reference internal" href="kernels.html#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel">falkon.kernels.kernel.Kernel</a></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">falkon.options.BaseOptions</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmm_cpu.fmm_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute kernel value on matrices X1 and X2: <code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">kernel(X1,</span> <span class="pre">X2)</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> – [N, D] array</p></li>
<li><p><strong>X2</strong> – [M, D] array</p></li>
<li><p><strong>kernel</strong> – Class representing the desired kernel function</p></li>
<li><p><strong>out</strong> – Array for storing the kernel output. If None, will be allocated within the function.</p></li>
<li><p><strong>opt</strong> – Basic options dictionary, used for determining available memory. Additionally, the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">no_single_kernel</span></code> option is used to determine the
accumulator data type.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><em>out</em> – [N, M] array. The kernel between X1 and X2.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fmm-cpu-sparse">
<h2>fmm_cpu_sparse<a class="headerlink" href="#fmm-cpu-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel calculation on the CPU for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmm_cpu.fmm_cpu_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmm_cpu.</code><code class="sig-name descname">fmm_cpu_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">kernel</span><span class="p">:</span> <span class="n"><a class="reference internal" href="kernels.html#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel">falkon.kernels.kernel.Kernel</a></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">falkon.options.BaseOptions</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmm_cpu.fmm_cpu_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fmm-cuda">
<h2>fmm_cuda<a class="headerlink" href="#fmm-cuda" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel calculation on GPUs.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmm_cuda.fmm_cuda">
<code class="sig-prename descclassname">falkon.mmv_ops.fmm_cuda.</code><code class="sig-name descname">fmm_cuda</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="p">:</span> <span class="n"><a class="reference internal" href="kernels.html#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel">falkon.kernels.kernel.Kernel</a></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmm_cuda.fmm_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>performs fnc(X1*X2’, X1, X2) in blocks on multiple GPUs</p>
</dd></dl>

</div>
<div class="section" id="fmm-cuda-sparse">
<h2>fmm_cuda_sparse<a class="headerlink" href="#fmm-cuda-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel calculation on GPUs for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmm_cuda.fmm_cuda_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmm_cuda.</code><code class="sig-name descname">fmm_cuda_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">kernel</span><span class="p">:</span> <span class="n"><a class="reference internal" href="kernels.html#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel">falkon.kernels.kernel.Kernel</a></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmm_cuda.fmm_cuda_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fmmv-cpu">
<h2>fmmv_cpu<a class="headerlink" href="#fmmv-cpu" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel-vector products on CPU.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cpu.fmmv_cpu">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cpu.</code><code class="sig-name descname">fmmv_cpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span></em>, <em class="sig-param"><span class="n">X2</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span></em>, <em class="sig-param"><span class="n">opt</span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.mmv_ops.fmmv_cpu.fmmv_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Blockwise kernel-vector product</p>
<p>This function computes <code class="docutils literal notranslate"><span class="pre">kernel(X1,</span> <span class="pre">X2)</span> <span class="pre">&#64;</span> <span class="pre">v</span></code> in a blockwise fashion, to avoid having the
whole N*M kernel matrix in memory at once.
Note that while the principle is that of matrix-vector product, <cite>v</cite> can have more than
one column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> – [N, D] array</p></li>
<li><p><strong>X2</strong> – [M, D] array</p></li>
<li><p><strong>v</strong> – [M, T] array</p></li>
<li><p><strong>kernel</strong> – Class representing the desired kernel function</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – [N, T] array for storing the kernel-vector product output.
If None, will be allocated within the function.</p></li>
<li><p><strong>opt</strong> – Basic options dictionary, used for determining available memory.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fmmv-cpu-sparse">
<h2>fmmv_cpu_sparse<a class="headerlink" href="#fmmv-cpu-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel-vector products on CPU for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cpu.fmmv_cpu_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cpu.</code><code class="sig-name descname">fmmv_cpu_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="p">:</span> <span class="n"><a class="reference internal" href="kernels.html#falkon.kernels.kernel.Kernel" title="falkon.kernels.kernel.Kernel">falkon.kernels.kernel.Kernel</a></span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">falkon.options.BaseOptions</span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.mmv_ops.fmmv_cpu.fmmv_cpu_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fdmmv-cpu">
<h2>fdmmv_cpu<a class="headerlink" href="#fdmmv-cpu" title="Permalink to this headline">¶</a></h2>
<p>Blocked double kernel-vector products on CPU.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cpu.fdmmv_cpu">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cpu.</code><code class="sig-name descname">fdmmv_cpu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span></em>, <em class="sig-param"><span class="n">X2</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">w</span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span></em>, <em class="sig-param"><span class="n">opt</span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.mmv_ops.fmmv_cpu.fdmmv_cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate a double kernel-vector product.</p>
<p>This function computes the following quantity: <code class="docutils literal notranslate"><span class="pre">kernel(X1,</span> <span class="pre">X2).T</span> <span class="pre">&#64;</span> <span class="pre">(kernel(X1,</span> <span class="pre">X2)</span> <span class="pre">&#64;</span> <span class="pre">v</span> <span class="pre">+</span> <span class="pre">w)</span></code>
Where one of <cite>v</cite> or <cite>w</cite> can be empty.
All arrays passed to this function must be 2-dimensional, although
the second dimension can be unitary.</p>
<p>The expression is not computed directly. We separate the computation
into smaller blocks so as to reduce the total memory consumption (the
large N*M kernel matrix is never wholly stored in RAM.)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X1</strong> – [N, D] array</p></li>
<li><p><strong>X2</strong> – [M, D] array</p></li>
<li><p><strong>v</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – [M, T] array. But note that at least one of v or w must be specified.</p></li>
<li><p><strong>w</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – [N, T] array. But note that at least one of v or w must be specified.</p></li>
<li><p><strong>kernel</strong> – Class representing the desired kernel function</p></li>
<li><p><strong>out</strong> (<em>torch.Tensor</em><em> or </em><em>None</em>) – [M, T] array for storing the kernel-vector product output.
If None, will be allocated within the function.</p></li>
<li><p><strong>opt</strong> – Basic options dictionary, used for determining available memory.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fdmmv-cpu-sparse">
<h2>fdmmv_cpu_sparse<a class="headerlink" href="#fdmmv-cpu-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked double kernel-vector products on CPU for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cpu.fdmmv_cpu_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cpu.</code><code class="sig-name descname">fdmmv_cpu_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">w</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#falkon.mmv_ops.fmmv_cpu.fdmmv_cpu_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fmmv-cuda">
<h2>fmmv_cuda<a class="headerlink" href="#fmmv-cuda" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel-vector products on GPUs.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cuda.fmmv_cuda">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cuda.</code><code class="sig-name descname">fmmv_cuda</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_cuda.fmmv_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>X1 : N x D
X2 : M x D
v  : M x T</p>
<p>performs  fnc(X1*X2’, X1, X2) * v   : N x T
in blocks on multiple GPUs</p>
</dd></dl>

</div>
<div class="section" id="fmmv-cuda-sparse">
<h2>fmmv_cuda_sparse<a class="headerlink" href="#fmmv-cuda-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked kernel-vector products on GPUs for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cuda.fmmv_cuda_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cuda.</code><code class="sig-name descname">fmmv_cuda_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_cuda.fmmv_cuda_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="fdmmv-cuda">
<h2>fdmmv_cuda<a class="headerlink" href="#fdmmv-cuda" title="Permalink to this headline">¶</a></h2>
<p>Blocked double kernel-vector products on GPUs.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cuda.fdmmv_cuda">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cuda.</code><code class="sig-name descname">fdmmv_cuda</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">w</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_cuda.fdmmv_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>X1 : N x D
X2 : M x D
v  : M x T
w  : N x T</p>
<p>performs fnc(X1*X2’, X1, X2)’ * ( fnc(X1*X2’, X1, X2) * v  +  w )  : M x T
in blocks on multiple GPUs</p>
<p>Assume all inputs have the same data type</p>
</dd></dl>

</div>
<div class="section" id="fdmmv-cuda-sparse">
<h2>fdmmv_cuda_sparse<a class="headerlink" href="#fdmmv-cuda-sparse" title="Permalink to this headline">¶</a></h2>
<p>Blocked double kernel-vector products on GPUs for sparse datasets.</p>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_cuda.fdmmv_cuda_sparse">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_cuda.</code><code class="sig-name descname">fdmmv_cuda_sparse</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X1</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">X2</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparse.html#falkon.sparse.sparse_tensor.SparseTensor" title="falkon.sparse.sparse_tensor.SparseTensor">falkon.sparse.sparse_tensor.SparseTensor</a></span></em>, <em class="sig-param"><span class="n">v</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">w</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">kernel</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>falkon.options.BaseOptions<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_cuda.fdmmv_cuda_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="incore-fmmv">
<h2>incore_fmmv<a class="headerlink" href="#incore-fmmv" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_incore.incore_fmmv">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_incore.</code><code class="sig-name descname">incore_fmmv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mat</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">vec</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">transpose</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions">falkon.options.FalkonOptions</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_incore.incore_fmmv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="incore-fdmmv">
<h2>incore_fdmmv<a class="headerlink" href="#incore-fdmmv" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="falkon.mmv_ops.fmmv_incore.incore_fdmmv">
<code class="sig-prename descclassname">falkon.mmv_ops.fmmv_incore.</code><code class="sig-name descname">incore_fdmmv</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mat</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">vec</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">opt</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="options.html#falkon.options.FalkonOptions" title="falkon.options.FalkonOptions">falkon.options.FalkonOptions</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="headerlink" href="#falkon.mmv_ops.fmmv_incore.incore_fdmmv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="sparse.html" class="btn btn-neutral float-right" title="falkon.sparse" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="outofcore.html" class="btn btn-neutral float-left" title="falkon.ooc_ops" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Giacomo Meanti

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>